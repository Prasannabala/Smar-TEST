# ============================================
# âš¡ Smar-Test - Docker Compose Configuration
# Smart + Test = Smartest Test Case Generation
# ============================================
#
# Starts Smar-Test app + Ollama local LLM.
#
# Quick start:
#   docker-compose up -d
#   # Then open Smar-Test at http://localhost:8501
#
# Pull a model into Ollama:
#   docker-compose exec ollama ollama pull qwen2.5:7b
#   docker-compose exec ollama ollama pull codellama:7b
#
# Cloud-only mode (no Ollama):
#   docker-compose up smar-test-app
#   # Configure OpenAI/Groq/HuggingFace/Anthropic in LLM Settings
#
# vLLM mode (high-performance local inference):
#   docker-compose --profile vllm up -d
#   # This starts vLLM server with GPU support
#   # Configure vLLM provider in LLM Settings
# ============================================

version: "3.8"

services:
  # --- Smar-Test Application ---
  smar-test-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: smar-test
    ports:
      - "8501:8501"
    environment:
      # Ollama connection (points to the ollama service)
      - OLLAMA_BASE_URL=http://ollama:11434
      # vLLM connection (points to the vllm service when enabled)
      - VLLM_SERVER_URL=http://vllm:8000
      # Optional: Cloud API keys (or configure in the UI)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - HF_API_TOKEN=${HF_API_TOKEN:-}
    volumes:
      # Persist database, settings, and exports across restarts
      - app-data:/app/data
    depends_on:
      ollama:
        condition: service_started
    restart: unless-stopped
    networks:
      - agent-network

  # --- Ollama Local LLM Server ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      # Persist downloaded models
      - ollama-models:/root/.ollama
    restart: unless-stopped
    networks:
      - agent-network
    # Uncomment below for GPU support (NVIDIA):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # --- vLLM High-Performance Inference Server (Optional) ---
  # Enable with: docker-compose --profile vllm up -d
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    profiles:
      - vllm
    ports:
      - "8000:8000"
    environment:
      # Model to serve (change as needed)
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
      # GPU memory utilization (0-1)
      - GPU_MEMORY_UTILIZATION=0.9
      # Number of GPUs for tensor parallelism
      - TENSOR_PARALLEL_SIZE=1
      # Data type (auto, float16, bfloat16)
      - DTYPE=auto
    volumes:
      # Cache HuggingFace models locally
      - vllm-models:/root/.cache/huggingface
    command:
      - --model
      - ${VLLM_MODEL:-meta-llama/Llama-3.1-8B-Instruct}
      - --gpu-memory-utilization
      - ${VLLM_GPU_MEMORY:-0.9}
      - --tensor-parallel-size
      - ${VLLM_TENSOR_PARALLEL:-1}
      - --dtype
      - ${VLLM_DTYPE:-auto}
      - --max-model-len
      - ${VLLM_MAX_MODEL_LEN:-4096}
      - --trust-remote-code
    restart: unless-stopped
    networks:
      - agent-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '4gb'  # Shared memory for better performance

volumes:
  app-data:
    driver: local
  ollama-models:
    driver: local
  vllm-models:
    driver: local

networks:
  agent-network:
    driver: bridge
